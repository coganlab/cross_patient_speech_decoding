{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "629c8e9894f680c6",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9de9524454a74a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T12:36:37.423150Z",
     "start_time": "2024-07-10T12:36:26.252451Z"
    }
   },
   "outputs": [],
   "source": [
    "# %% Imports\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint, LearningRateMonitor\n",
    "from pytorch_lightning.utilities.model_summary import summarize\n",
    "import torch\n",
    "import numpy as np\n",
    "from data_utils.datamodules import SimpleMicroDataModule, AlignedMicroDataModule\n",
    "from models import CNNTransformer, Transformer, TCN_classifier, TemporalConvRNN, Seq2SeqRNN\n",
    "import data_utils.augmentations as augs\n",
    "import csv\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from alignment import alignment_utils as utils\n",
    "from alignment.AlignCCA import AlignCCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae402b0e2a9fc07",
   "metadata": {},
   "source": [
    "# Define data module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a46a71918adb132d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T12:36:42.704611Z",
     "start_time": "2024-07-10T12:36:37.423150Z"
    }
   },
   "outputs": [],
   "source": [
    "data_filename = os.path.expanduser('~/data/pt_decoding_data_S62.pkl')\n",
    "# data_filename = ('../data/pt_decoding_data_S62.pkl')\n",
    "pt_data = utils.load_pkl(data_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ced55da2478416b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T12:36:42.715275Z",
     "start_time": "2024-07-10T12:36:42.704611Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(144, 200, 111), (144,), (144, 3)]\n",
      "[[(148, 200, 111), (148,), (148, 3)], [(151, 200, 63), (151,), (151, 3)], [(46, 200, 149), (46,), (46, 3)], [(151, 200, 74), (151,), (151, 3)], [(137, 200, 144), (137,), (137, 3)], [(141, 200, 171), (141,), (141, 3)], [(178, 200, 201), (178,), (178, 3)]]\n"
     ]
    }
   ],
   "source": [
    "pt = 'S14'\n",
    "p_ind = 1\n",
    "lab_type = 'phon'\n",
    "algn_type = 'phon_seq'\n",
    "tar_data, pre_data = utils.decoding_data_from_dict(pt_data, pt, p_ind,\n",
    "                                                   lab_type=lab_type,\n",
    "                                                   algn_type=algn_type)\n",
    "print([d.shape for d in tar_data])\n",
    "print([[d.shape for d in p] for p in pre_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b30a66092a23064f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T12:37:23.778265Z",
     "start_time": "2024-07-10T12:36:42.715275Z"
    }
   },
   "outputs": [],
   "source": [
    "# fold_data_path = '.'\n",
    "\n",
    "fs = 200 # Hz\n",
    "# augmentations = [augs.time_warping, augs.time_masking, augs.time_shifting, augs.noise_jitter, augs.scaling]\n",
    "augmentations = [augs.time_shifting, augs.noise_jitter, augs.scaling]\n",
    "# augmentations = None\n",
    "# data = torch.rand(n_samples, n_timepoints, n_features)\n",
    "# labels = torch.randint(0, 9, (n_samples,))\n",
    "# data = torch.Tensor(all_pt_dict['S14']['X1'])\n",
    "# labels = torch.Tensor(all_pt_dict['S14']['y1']).long() - 1\n",
    "data = torch.Tensor(tar_data[0])\n",
    "labels = torch.Tensor(tar_data[1]).long().unsqueeze(1) - 1\n",
    "align_labels = torch.Tensor(tar_data[2]).long() - 1\n",
    "# pool_data = [(torch.Tensor(p[0]), torch.Tensor(p[1]).long().unsqueeze(1) - 1, torch.Tensor(p[2]).long() - 1) for p in pre_data]\n",
    "pool_data = [(torch.Tensor(p[0]), torch.Tensor(p[2]).long() - 1, torch.Tensor(p[2]).long() - 1) for p in pre_data]  # for seq2seq RNN\n",
    "# data = torch.Tensor(all_pt_dict['S14']['X_collapsed'])\n",
    "# labels = torch.Tensor(all_pt_dict['S14']['y_phon_collapsed']).long() - 1\n",
    "\n",
    "# create the data module\n",
    "batch_size = 5000\n",
    "n_folds = 20\n",
    "val_size = 0.1\n",
    "\n",
    "# context_prefix = 'ptSpecific'\n",
    "context_prefix = 'pooled'\n",
    "fold_data_path = os.path.expanduser('~/workspace/transformer_data/datamodules/') + context_prefix\n",
    "\n",
    "# dm = SimpleMicroDataModule(data, labels, batch_size=batch_size, folds=n_folds,\n",
    "#                            val_size=val_size, augmentations=augmentations, data_path=os.path.expanduser('~/workspace/transformer_data/pt_specific'))\n",
    "## modification of labels for seq2seq RNN ###\n",
    "# dm = SimpleMicroDataModule(data, align_labels, batch_size=batch_size, folds=n_folds,\n",
    "#                            val_size=val_size, augmentations=augmentations, data_path=fold_data_path)\n",
    "\n",
    "\n",
    "# # dm = AlignedMicroDataModule(data, labels, align_labels, pool_data, AlignCCA,\n",
    "# #                             batch_size=batch_size, folds=n_folds, val_size=val_size,\n",
    "# #                             augmentations=augmentations, data_path=fold_data_path)\n",
    "dm = AlignedMicroDataModule(data, align_labels, align_labels, pool_data, AlignCCA,\n",
    "                            batch_size=batch_size, folds=n_folds, val_size=val_size,\n",
    "                            augmentations=augmentations, data_path=fold_data_path)\n",
    "# dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "98fad2f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4300, 3])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm.train_dataloader().dataset.tensors[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1e18287490630e",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ffdeb3bdad199307",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T12:37:23.785482Z",
     "start_time": "2024-07-10T12:37:23.778265Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  | Name          | Type             | Params | Mode \n",
      "-----------------------------------------------------------\n",
      "0 | criterion     | CrossEntropyLoss | 0      | train\n",
      "1 | temporal_conv | TemporalConv     | 111 K  | train\n",
      "2 | encoder       | EncoderRNN       | 6.3 M  | train\n",
      "3 | decoder       | DecoderRNN       | 1.5 M  | train\n",
      "-----------------------------------------------------------\n",
      "7.9 M     Trainable params\n",
      "0         Non-trainable params\n",
      "7.9 M     Total params\n",
      "31.743    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hpc/group/coganlab/zms14/miniconda3/envs/micro_decode/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.utilities.model_summary import summarize\n",
    "\n",
    "\n",
    "gclip_val = 0.5\n",
    "\n",
    "##### CNN TRANSFORMER #####\n",
    "\n",
    "# # model parameters\n",
    "# in_channels = data.shape[-1]\n",
    "# num_classes = 9\n",
    "# d_model = 128\n",
    "# # d_model = data.shape[-1]\n",
    "# kernel_time = 50  # ms\n",
    "# kernel_size = int(kernel_time * fs / 1000)  # kernel length in samples\n",
    "# stride_time = 25  # ms\n",
    "# stride = int(stride_time * fs / 1000)  # stride length in samples\n",
    "# padding = 0\n",
    "# n_head = 8\n",
    "# num_layers = 2\n",
    "# dim_fc = 64\n",
    "# # dim_fc = [128, 256, 128, 64]\n",
    "# cnn_dropout = 0.3\n",
    "# tform_dropout = 0.4\n",
    "# learning_rate = 5e-4\n",
    "# l2_reg = 1e-5\n",
    "# gclip_val = 0.5\n",
    "# activ = True\n",
    "\n",
    "# sum_model = CNNTransformer(in_channels, num_classes, d_model, kernel_size, stride, padding,\n",
    "#                            n_head, num_layers, dim_fc, cnn_dropout, tform_dropout, learning_rate)\n",
    "\n",
    "\n",
    "##### Temporal CNN classifier #####\n",
    "\n",
    "# # model parameters\n",
    "# in_channels = data.shape[-1]\n",
    "# num_classes = 9\n",
    "# kernel_time = 50  # ms\n",
    "# kernel_size = int(kernel_time * fs / 1000)  # kernel length in samples\n",
    "# stride_time = 25  # ms\n",
    "# stride = int(stride_time * fs / 1000)  # stride length in samples\n",
    "# padding = 0\n",
    "# dim_fc = [128, 256, 128, 64]\n",
    "# cnn_dropout = 0.3\n",
    "# learning_rate = 5e-4\n",
    "# l2_reg = 1e-5\n",
    "# gclip_val = 0.5\n",
    "# activ = True\n",
    "\n",
    "# sum_model = TCN_classifier(in_channels, num_classes, dim_fc, kernel_size, stride, padding,\n",
    "#                            cnn_dropout, learning_rate, l2_reg)\n",
    "\n",
    "\n",
    "##### Temporal CNN GRU #####\n",
    "\n",
    "# # model parameters\n",
    "# in_channels = data.shape[-1]\n",
    "# num_classes = 9\n",
    "# n_filters = 100\n",
    "# # d_model = data.shape[-1]\n",
    "# kernel_time = 50  # ms\n",
    "# kernel_size = int(kernel_time * fs / 1000)  # kernel length in samples\n",
    "# stride_time = 25  # ms\n",
    "# stride = int(stride_time * fs / 1000)  # stride length in samples\n",
    "# padding = 0\n",
    "# n_layers = 1\n",
    "# hidden_size = 500\n",
    "# dim_fc = [128, 64]\n",
    "# cnn_dropout = 0.3\n",
    "# rnn_dropout = 0.4\n",
    "# learning_rate = 5e-4\n",
    "# l2_reg = 1e-5\n",
    "# activ = True\n",
    "\n",
    "# sum_model = TemporalConvRNN(in_channels, n_filters, num_classes, hidden_size, n_layers,\n",
    "#                             kernel_size, dim_fc, stride, padding, cnn_dropout,\n",
    "#                             rnn_dropout, learning_rate, l2_reg)\n",
    "\n",
    "\n",
    "\n",
    "##### Seq2Seq RNN #####\n",
    "# model parameters\n",
    "in_channels = data.shape[-1]\n",
    "num_classes = 9\n",
    "n_filters = 100\n",
    "# d_model = data.shape[-1]\n",
    "kernel_time = 50  # ms\n",
    "kernel_size = int(kernel_time * fs / 1000)  # kernel length in samples\n",
    "stride_time = 50  # ms\n",
    "stride = int(stride_time * fs / 1000)  # stride length in samples\n",
    "padding = 0\n",
    "n_enc_layers = 2\n",
    "n_dec_layers = 1\n",
    "hidden_size = 500\n",
    "cnn_dropout = 0.3\n",
    "rnn_dropout = 0.3\n",
    "learning_rate = 1e-4\n",
    "l2_reg = 1e-5\n",
    "activ = False\n",
    "model_type = 'gru'\n",
    "\n",
    "sum_model = Seq2SeqRNN(in_channels, n_filters, hidden_size, num_classes, n_enc_layers,\n",
    "                       n_dec_layers, kernel_size, stride, padding, cnn_dropout,\n",
    "                       rnn_dropout, model_type, learning_rate, l2_reg)\n",
    "\n",
    "print(summarize(sum_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad31833df23d804b",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1b5454c4b162770f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T12:37:24.423208Z",
     "start_time": "2024-07-10T12:37:24.417362Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", \".*does not have many workers.*\")\n",
    "warnings.filterwarnings(\"ignore\", \"The number of training batches.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1b1f762b11299743",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T12:37:24.431450Z",
     "start_time": "2024-07-10T12:37:24.423208Z"
    }
   },
   "outputs": [],
   "source": [
    "# instantiate the trainer\n",
    "max_epochs = 500\n",
    "# es_pat = max_steps // 20\n",
    "# max_steps = 500\n",
    "es_pat = 50\n",
    "warmup = 100\n",
    "# callbacks = [EarlyStopping(monitor='val_loss', patience=10)]\n",
    "log_dir = os.path.expanduser('~/workspace/transformer_data/transformer_logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddb5ecc37742fee3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T12:37:24.439028Z",
     "start_time": "2024-07-10T12:37:24.431450Z"
    }
   },
   "outputs": [],
   "source": [
    "# class MetricCollector(L.Callback):\n",
    "#     def __init__(self):\n",
    "#         self.metrics = {}\n",
    "#     \n",
    "#     def on_validation_epoch_end(self, trainer, pl_module):\n",
    "#         self.metrics['val_loss'] = trainer.logger.metrics['val_loss']\n",
    "#         self.metrics['val_acc'] = trainer.logger.metrics['val_acc']\n",
    "#     \n",
    "#     def on_test_epoch_end(self, trainer, pl_module):\n",
    "#         self.metrics['test_loss'] = trainer.logger.metrics['test_loss']\n",
    "#         self.metrics['test_acc'] = trainer.logger.metrics['test_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d553c30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.functional.classification import multiclass_confusion_matrix\n",
    "\n",
    "def cmat_acc(y_hat, y, num_classes):\n",
    "    y_pred = torch.argmax(y_hat, dim=1)\n",
    "    cmat = multiclass_confusion_matrix(y_pred, y, num_classes)\n",
    "    acc_cmat = cmat.diag().sum() / cmat.sum()\n",
    "    return acc_cmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30910d8dbfaa0aa8",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-10T12:37:24.440539Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Setting up data module for iteration 1 #####\n"
     ]
    }
   ],
   "source": [
    "acc_dir = os.path.expanduser('~/workspace/transformer_data/accs/')\n",
    "\n",
    "# train the model\n",
    "n_iters = 20\n",
    "iter_accs = []\n",
    "for i in range(n_iters):\n",
    "    print(f'##### Setting up data module for iteration {i+1} #####')\n",
    "    dm.setup()\n",
    "    \n",
    "    fold_accs = []\n",
    "    # y_pred_all = []\n",
    "    # y_test_all = []\n",
    "    for fold in range(n_folds):\n",
    "        # if fold > 1:\n",
    "        #     break\n",
    "        dm.set_fold(fold)\n",
    "        # print(dm.current_fold)\n",
    "        \n",
    "        # instantiate the model\n",
    "        in_channels = dm.get_data_shape()[-1]\n",
    "        # print(in_channels)\n",
    "        # model = CNNTransformer(in_channels, num_classes, d_model, kernel_size, stride, padding,\n",
    "        #                        n_head, num_layers, dim_fc, cnn_dropout, tform_dropout, learning_rate,\n",
    "        #                        warmup, max_steps, l2_reg, activation=activ)\n",
    "        # model = TCN_classifier(in_channels, num_classes, dim_fc, kernel_size, stride, padding,\n",
    "        #                            cnn_dropout, learning_rate, l2_reg, activation=activ)\n",
    "        # model = TemporalConvRNN(in_channels, n_filters, num_classes, hidden_size, n_layers,\n",
    "                                # kernel_size, dim_fc, stride, padding, cnn_dropout,\n",
    "                                # rnn_dropout, learning_rate, l2_reg, activation=activ,\n",
    "                                # decay_iters=max_epochs)\n",
    "        model = Seq2SeqRNN(in_channels, n_filters, hidden_size, num_classes, n_enc_layers,\n",
    "                           n_dec_layers, kernel_size, stride, padding, cnn_dropout, rnn_dropout, model_type,\n",
    "                           learning_rate, l2_reg, activation=activ, decay_iters=max_epochs)\n",
    "        \n",
    "        # model.current_fold = fold\n",
    "        callbacks = [\n",
    "            # ModelCheckpoint(monitor='val_loss', mode='min'),\n",
    "            ModelCheckpoint(monitor='val_acc', mode='max'),\n",
    "            # EarlyStopping(monitor='val_loss', patience=es_pat),\n",
    "            LearningRateMonitor(logging_interval='epoch'),\n",
    "            ]\n",
    "        trainer = L.Trainer(default_root_dir=log_dir,\n",
    "                            max_epochs=max_epochs,\n",
    "                            # max_steps=max_steps,\n",
    "                            gradient_clip_val=gclip_val,\n",
    "                            accelerator='auto',\n",
    "                            callbacks=callbacks,\n",
    "                            logger=True,\n",
    "                            enable_model_summary=False,\n",
    "                            enable_progress_bar=False,\n",
    "                           )\n",
    "        # # trainer.fit(model, dm)\n",
    "        trainer.fit(model=model, train_dataloaders=dm.train_dataloader(), val_dataloaders=dm.val_dataloader())\n",
    "        print(trainer.logged_metrics)\n",
    "        # print(trainer.callback_metrics)\n",
    "        # print the training metrics from the best model checkpoint\n",
    "        # print(f'Fold {fold} best model metrics:')\n",
    "        # print(trainer.checkpoint_callback.best_model_score)\n",
    "\n",
    "\n",
    "        # trainer.test(model, dm)\n",
    "        # model = CNNTransformer.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
    "        trainer.test(model=model, dataloaders=dm.test_dataloader(), ckpt_path='best')\n",
    "        # trainer.test(dataloaders=dm.test_dataloader(), ckpt_path='best')\n",
    "\n",
    "        # test_pred = model(dm.test_dataloader().dataset.tensors[0])\n",
    "        # test_pred = trainer.predict(model, dm.test_dataloader(), ckpt_path='best)[0]\n",
    "        # test_pred = torch.argmax(test_pred, dim=1)\n",
    "        # print(test_pred)\n",
    "        # y_pred_all.extend(test_pred)\n",
    "        # y_test_all.extend(dm.test_dataloader().dataset.tensors[1])\n",
    "        \n",
    "        fold_accs.append(trainer.logged_metrics['test_acc'])\n",
    "    \n",
    "        # save loss information\n",
    "        # loss_dict = trainer.logger.metrics\n",
    "        # loss_dict['fold'] = fold\n",
    "        # loss_dict['model'] = model\n",
    "    # acc = cmat_acc(torch.stack(y_pred_all), torch.stack(y_test_all), num_classes)\n",
    "    # print(acc)\n",
    "    # iter_accs.append(acc)\n",
    "    # print(f'Averaged accuracy: {sum(fold_accs) / len(fold_accs)}')\n",
    "    iter_accs.append(fold_accs)\n",
    "    with open(os.path.join(acc_dir, f'{context_prefix}/{pt}_{context_prefix}_seq2seq_rnn_accs_iter{i+1}.csv'), 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(iter_accs)\n",
    "    print(np.mean(fold_accs))\n",
    "# print(sum(iter_accs) / len(iter_accs), iter_accs)\n",
    "print(iter_accs)\n",
    "with open(os.path.join(acc_dir, f'{context_prefix}/{pt}_{context_prefix}_seq2seq_rnn_accs.csv'), 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(iter_accs)\n",
    "np.save(os.path.join(acc_dir, f'{context_prefix}/{pt}_{context_prefix}_seq2seq_rnn_accs.npy'), np.array(iter_accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1e334142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 20)\n",
      "[0.44136906]\n",
      "0.44136906\n"
     ]
    }
   ],
   "source": [
    "iter_accs = np.load(os.path.join(acc_dir, f'{context_prefix}/{pt}_{context_prefix}_seq2seq_rnn_accs.npy'))\n",
    "print(iter_accs.shape)\n",
    "print(iter_accs.mean(axis=1))\n",
    "print(iter_accs.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9a9b87d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# from torch import tensor\n",
    "# test = [[tensor(0.5000), tensor(0.3636), tensor(0.2727), tensor(0.4545), tensor(0.3182), tensor(0.4545), tensor(0.4091), tensor(0.2273), tensor(0.2727), tensor(0.4091), tensor(0.2727), tensor(0.3182), tensor(0.3333), tensor(0.3333), tensor(0.4762), tensor(0.3333), tensor(0.2857), tensor(0.3810), tensor(0.3810), tensor(0.3810)]]\n",
    "# plt.boxplot(iter_accs.mean(axis=1))\n",
    "# sns.boxplot(data=iter_accs.mean(axis=1))\n",
    "# sns.swarmplot(data=iter_accs.mean(axis=1), color='black')\n",
    "sns.boxplot(data=iter_accs.T)\n",
    "sns.swarmplot(data=iter_accs.T, color='black')\n",
    "plt.axhline(y=1/9, color='gray', linestyle='--')\n",
    "plt.ylim(0, None)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc82690e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35327384"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "np.mean(fold_accs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "micro_decode",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
