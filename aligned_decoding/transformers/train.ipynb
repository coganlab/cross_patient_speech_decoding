{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "629c8e9894f680c6",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9de9524454a74a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T12:36:37.423150Z",
     "start_time": "2024-07-10T12:36:26.252451Z"
    }
   },
   "outputs": [],
   "source": [
    "# %% Imports\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.callbacks.model_checkpoint import ModelCheckpoint\n",
    "import torch\n",
    "from datamodules import SimpleMicroDataModule, AlignedMicroDataModule\n",
    "from models import CNNTransformer, Transformer\n",
    "import augmentations as augs\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from alignment import alignment_utils as utils\n",
    "from alignment.AlignCCA import AlignCCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae402b0e2a9fc07",
   "metadata": {},
   "source": [
    "# Define data module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a46a71918adb132d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T12:36:42.704611Z",
     "start_time": "2024-07-10T12:36:37.423150Z"
    }
   },
   "outputs": [],
   "source": [
    "data_filename = '../data/pt_decoding_data_S62.pkl'\n",
    "pt_data = utils.load_pkl(data_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ced55da2478416b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T12:36:42.715275Z",
     "start_time": "2024-07-10T12:36:42.704611Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(432, 200, 111), (432,), (432, 3)]\n",
      "[[(444, 200, 111), (444,), (444, 3)], [(453, 200, 63), (453,), (453, 3)], [(138, 200, 149), (138,), (138, 3)], [(453, 200, 74), (453,), (453, 3)], [(411, 200, 144), (411,), (411, 3)], [(423, 200, 171), (423,), (423, 3)], [(534, 200, 201), (534,), (534, 3)]]\n"
     ]
    }
   ],
   "source": [
    "pt = 'S14'\n",
    "p_ind = -1\n",
    "lab_type = 'phon'\n",
    "algn_type = 'phon_seq'\n",
    "tar_data, pre_data = utils.decoding_data_from_dict(pt_data, pt, p_ind,\n",
    "                                                   lab_type=lab_type,\n",
    "                                                   algn_type=algn_type)\n",
    "print([d.shape for d in tar_data])\n",
    "print([[d.shape for d in p] for p in pre_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b30a66092a23064f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T12:37:23.778265Z",
     "start_time": "2024-07-10T12:36:42.715275Z"
    }
   },
   "outputs": [],
   "source": [
    "# dummy data\n",
    "# n_samples = 144\n",
    "# n_timepoints = 200\n",
    "# n_features = 111\n",
    "fs = 200 # Hz\n",
    "augmentations = [augs.time_warping, augs.time_masking, augs.time_shifting, augs.noise_jitter, augs.scaling]\n",
    "# data = torch.rand(n_samples, n_timepoints, n_features)\n",
    "# labels = torch.randint(0, 9, (n_samples,))\n",
    "# data = torch.Tensor(all_pt_dict['S14']['X1'])\n",
    "# labels = torch.Tensor(all_pt_dict['S14']['y1']).long() - 1\n",
    "data = torch.Tensor(tar_data[0])\n",
    "labels = torch.Tensor(tar_data[1]).long() - 1\n",
    "align_labels = torch.Tensor(tar_data[2]).long() - 1\n",
    "pool_data = [(torch.Tensor(p[0]), torch.Tensor(p[1]).long() - 1, torch.Tensor(p[2]).long() - 1) for p in pre_data]\n",
    "# data = torch.Tensor(all_pt_dict['S14']['X_collapsed'])\n",
    "# labels = torch.Tensor(all_pt_dict['S14']['y_phon_collapsed']).long() - 1\n",
    "\n",
    "# create the data module\n",
    "batch_size = -1\n",
    "n_folds = 20\n",
    "val_size = 0.1\n",
    "# dm = SimpleMicroDataModule(data, labels, batch_size=batch_size, folds=n_folds,\n",
    "#                            val_size=val_size, augmentations=augmentations)\n",
    "dm = AlignedMicroDataModule(data, labels, align_labels, pool_data, AlignCCA,\n",
    "                            batch_size=batch_size, folds=n_folds, val_size=val_size,\n",
    "                            augmentations=augmentations)\n",
    "# dm.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1e18287490630e",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ffdeb3bdad199307",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T12:37:23.785482Z",
     "start_time": "2024-07-10T12:37:23.778265Z"
    }
   },
   "outputs": [],
   "source": [
    "# model parameters\n",
    "in_channels = data.shape[-1]\n",
    "num_classes = 9\n",
    "d_model = 64\n",
    "# d_model = data.shape[-1]\n",
    "kernel_time = 50  # ms\n",
    "kernel_size = int(kernel_time * fs / 1000)  # kernel length in samples\n",
    "stride_time = 50  # ms\n",
    "stride = int(stride_time * fs / 1000)  # stride length in samples\n",
    "padding = 0\n",
    "n_head = 2\n",
    "num_layers = 2\n",
    "dim_fc = 128\n",
    "dropout = 0.4\n",
    "learning_rate = 5e-4\n",
    "l2_reg = 1e-5\n",
    "gclip_val = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc6e3e0a4501e5a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T12:37:24.327137Z",
     "start_time": "2024-07-10T12:37:23.785482Z"
    }
   },
   "outputs": [],
   "source": [
    "from pytorch_lightning.utilities.model_summary import summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8ee2f751323d2b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T12:37:24.417362Z",
     "start_time": "2024-07-10T12:37:24.328723Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  | Name                | Type               | Params | Mode \n",
      "-------------------------------------------------------------------\n",
      "0 | temporal_conv       | TemporalConv       | 71.2 K | train\n",
      "1 | positional_encoding | PositionalEncoding | 0      | train\n",
      "2 | transformer_encoder | TransformerEncoder | 66.9 K | train\n",
      "3 | fc                  | Linear             | 585    | train\n",
      "4 | criterion           | CrossEntropyLoss   | 0      | train\n",
      "-------------------------------------------------------------------\n",
      "138 K     Trainable params\n",
      "0         Non-trainable params\n",
      "138 K     Total params\n",
      "0.555     Total estimated model params size (MB)\n"
     ]
    }
   ],
   "source": [
    "sum_model = CNNTransformer(in_channels, num_classes, d_model, kernel_size, stride, padding,\n",
    "                           n_head, num_layers, dim_fc, dropout, learning_rate)\n",
    "print(summarize(sum_model))\n",
    "# sum_model = Transformer(in_channels, num_classes, d_model, kernel_size, stride, padding,\n",
    "#                            n_head, num_layers, dim_fc, dropout, learning_rate)\n",
    "# print(summarize(sum_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad31833df23d804b",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b5454c4b162770f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T12:37:24.423208Z",
     "start_time": "2024-07-10T12:37:24.417362Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", \".*does not have many workers.*\")\n",
    "warnings.filterwarnings(\"ignore\", \"The number of training batches.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b1f762b11299743",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T12:37:24.431450Z",
     "start_time": "2024-07-10T12:37:24.423208Z"
    }
   },
   "outputs": [],
   "source": [
    "# instantiate the trainer\n",
    "max_epochs = 500\n",
    "es_pat = max_epochs // 20\n",
    "# callbacks = [EarlyStopping(monitor='val_loss', patience=10)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ddb5ecc37742fee3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T12:37:24.439028Z",
     "start_time": "2024-07-10T12:37:24.431450Z"
    }
   },
   "outputs": [],
   "source": [
    "# class MetricCollector(L.Callback):\n",
    "#     def __init__(self):\n",
    "#         self.metrics = {}\n",
    "#     \n",
    "#     def on_validation_epoch_end(self, trainer, pl_module):\n",
    "#         self.metrics['val_loss'] = trainer.logger.metrics['val_loss']\n",
    "#         self.metrics['val_acc'] = trainer.logger.metrics['val_acc']\n",
    "#     \n",
    "#     def on_test_epoch_end(self, trainer, pl_module):\n",
    "#         self.metrics['test_loss'] = trainer.logger.metrics['test_loss']\n",
    "#         self.metrics['test_acc'] = trainer.logger.metrics['test_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "30910d8dbfaa0aa8",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-10T12:37:24.440539Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_loss': tensor(1.0982), 'train_acc': tensor(0.6176), 'val_loss': tensor(2.1682), 'val_acc': tensor(0.3415)}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.22727273404598236    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    2.7274084091186523     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.22727273404598236   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   2.7274084091186523    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_loss': tensor(1.1606), 'train_acc': tensor(0.5893), 'val_loss': tensor(2.2657), 'val_acc': tensor(0.2439)}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.13636364042758942    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    2.5273454189300537     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.13636364042758942   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   2.5273454189300537    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_loss': tensor(0.9930), 'train_acc': tensor(0.6593), 'val_loss': tensor(2.1782), 'val_acc': tensor(0.2927)}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.40909090638160706    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    2.0313162803649902     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.40909090638160706   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   2.0313162803649902    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_loss': tensor(1.1024), 'train_acc': tensor(0.6088), 'val_loss': tensor(2.2705), 'val_acc': tensor(0.4146)}\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 13.7 MiB for an array with shape (27600, 65) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m trainer\u001b[38;5;241m.\u001b[39mfit(model, dm)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(trainer\u001b[38;5;241m.\u001b[39mlogged_metrics)\n\u001b[1;32m---> 29\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m fold_accs\u001b[38;5;241m.\u001b[39mappend(trainer\u001b[38;5;241m.\u001b[39mlogged_metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_acc\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# save loss information\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# loss_dict = trainer.logger.metrics\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# loss_dict['fold'] = fold\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# loss_dict['model'] = model\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\zms14\\AppData\\Local\\miniconda3\\envs\\micro_decode\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:753\u001b[0m, in \u001b[0;36mTrainer.test\u001b[1;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[0;32m    751\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 753\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    754\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_test_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\n\u001b[0;32m    755\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\zms14\\AppData\\Local\\miniconda3\\envs\\micro_decode\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[0;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[1;32mc:\\Users\\zms14\\AppData\\Local\\miniconda3\\envs\\micro_decode\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:793\u001b[0m, in \u001b[0;36mTrainer._test_impl\u001b[1;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[0;32m    789\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    790\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    791\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn, ckpt_path, model_provided\u001b[38;5;241m=\u001b[39mmodel_provided, model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    792\u001b[0m )\n\u001b[1;32m--> 793\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[38;5;66;03m# remove the tensors from the test results\u001b[39;00m\n\u001b[0;32m    795\u001b[0m results \u001b[38;5;241m=\u001b[39m convert_tensors_to_scalars(results)\n",
      "File \u001b[1;32mc:\\Users\\zms14\\AppData\\Local\\miniconda3\\envs\\micro_decode\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:948\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    945\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: preparing data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    946\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_connector\u001b[38;5;241m.\u001b[39mprepare_data()\n\u001b[1;32m--> 948\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_setup_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# allow user to set up LightningModule in accelerator environment\u001b[39;00m\n\u001b[0;32m    949\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: configuring model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    950\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_configure_model(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\zms14\\AppData\\Local\\miniconda3\\envs\\micro_decode\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:94\u001b[0m, in \u001b[0;36m_call_setup_hook\u001b[1;34m(trainer)\u001b[0m\n\u001b[0;32m     91\u001b[0m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mbarrier(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpre_setup\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mdatamodule \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 94\u001b[0m     \u001b[43m_call_lightning_datamodule_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msetup\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m _call_callback_hooks(trainer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msetup\u001b[39m\u001b[38;5;124m\"\u001b[39m, stage\u001b[38;5;241m=\u001b[39mfn)\n\u001b[0;32m     96\u001b[0m _call_lightning_module_hook(trainer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msetup\u001b[39m\u001b[38;5;124m\"\u001b[39m, stage\u001b[38;5;241m=\u001b[39mfn)\n",
      "File \u001b[1;32mc:\\Users\\zms14\\AppData\\Local\\miniconda3\\envs\\micro_decode\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:181\u001b[0m, in \u001b[0;36m_call_lightning_datamodule_hook\u001b[1;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(fn):\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningDataModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mdatamodule\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 181\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mf:\\repos\\cross_patient_micro\\cross_patient_speech_decoding\\aligned_decoding\\transformers\\datamodules.py:152\u001b[0m, in \u001b[0;36mAlignedMicroDataModule.setup\u001b[1;34m(self, stage)\u001b[0m\n\u001b[0;32m    137\u001b[0m     val_data, val_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;66;03m# aug_data = torch.Tensor([])\u001b[39;00m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;66;03m# aug_labels = torch.Tensor([]).long()\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;66;03m# aug_align_labels = torch.Tensor([]).long()\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    149\u001b[0m \n\u001b[0;32m    150\u001b[0m \u001b[38;5;66;03m# align pooled data to current data\u001b[39;00m\n\u001b[0;32m    151\u001b[0m train_data, train_labels, dim_red \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 152\u001b[0m     \u001b[43mprocess_aligner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malign_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpool_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malgner\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    154\u001b[0m \u001b[38;5;66;03m# train_data, train_labels, dim_red = (\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;66;03m#     process_aligner(aug_data, aug_labels, aug_align_labels,\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;66;03m#                     self.pool_data, self.algner))\u001b[39;00m\n\u001b[0;32m    158\u001b[0m aug_data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor([])\n",
      "File \u001b[1;32mf:\\repos\\cross_patient_micro\\cross_patient_speech_decoding\\aligned_decoding\\transformers\\datamodules.py:229\u001b[0m, in \u001b[0;36mprocess_aligner\u001b[1;34m(X, y, y_align, pool_data, algner, n_components)\u001b[0m\n\u001b[0;32m    226\u001b[0m X_tar_r \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    228\u001b[0m \u001b[38;5;66;03m# reduce dimensionality of cross-patient data\u001b[39;00m\n\u001b[1;32m--> 229\u001b[0m X_cross_dr \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mPCA\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_components\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_components\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[43m              \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mX_cross_r\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;66;03m# reduce dimensionality of target data, saving dim. red. object for\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;66;03m# test set\u001b[39;00m\n\u001b[0;32m    234\u001b[0m tar_dr \u001b[38;5;241m=\u001b[39m PCA(n_components\u001b[38;5;241m=\u001b[39mn_components)\n",
      "File \u001b[1;32mf:\\repos\\cross_patient_micro\\cross_patient_speech_decoding\\aligned_decoding\\transformers\\datamodules.py:229\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    226\u001b[0m X_tar_r \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    228\u001b[0m \u001b[38;5;66;03m# reduce dimensionality of cross-patient data\u001b[39;00m\n\u001b[1;32m--> 229\u001b[0m X_cross_dr \u001b[38;5;241m=\u001b[39m [\u001b[43mPCA\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_components\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_components\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    230\u001b[0m               \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m X_cross_r]\n\u001b[0;32m    232\u001b[0m \u001b[38;5;66;03m# reduce dimensionality of target data, saving dim. red. object for\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;66;03m# test set\u001b[39;00m\n\u001b[0;32m    234\u001b[0m tar_dr \u001b[38;5;241m=\u001b[39m PCA(n_components\u001b[38;5;241m=\u001b[39mn_components)\n",
      "File \u001b[1;32mc:\\Users\\zms14\\AppData\\Local\\miniconda3\\envs\\micro_decode\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:313\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 313\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    315\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    316\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    317\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    318\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    319\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\zms14\\AppData\\Local\\miniconda3\\envs\\micro_decode\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\zms14\\AppData\\Local\\miniconda3\\envs\\micro_decode\\Lib\\site-packages\\sklearn\\decomposition\\_pca.py:487\u001b[0m, in \u001b[0;36mPCA.fit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    485\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m U\n\u001b[0;32m    486\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# solver=\"covariance_eigh\" does not compute U at fit time.\u001b[39;00m\n\u001b[1;32m--> 487\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_is_centered\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_is_centered\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\zms14\\AppData\\Local\\miniconda3\\envs\\micro_decode\\Lib\\site-packages\\sklearn\\decomposition\\_base.py:149\u001b[0m, in \u001b[0;36m_BasePCA._transform\u001b[1;34m(self, X, xp, x_is_centered)\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, xp, x_is_centered\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 149\u001b[0m     X_transformed \u001b[38;5;241m=\u001b[39m \u001b[43mX\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomponents_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x_is_centered:\n\u001b[0;32m    151\u001b[0m         \u001b[38;5;66;03m# Apply the centering after the projection.\u001b[39;00m\n\u001b[0;32m    152\u001b[0m         \u001b[38;5;66;03m# For dense X this avoids copying or mutating the data passed by\u001b[39;00m\n\u001b[0;32m    153\u001b[0m         \u001b[38;5;66;03m# the caller.\u001b[39;00m\n\u001b[0;32m    154\u001b[0m         \u001b[38;5;66;03m# For sparse X it keeps sparsity and avoids having to wrap X into\u001b[39;00m\n\u001b[0;32m    155\u001b[0m         \u001b[38;5;66;03m# a linear operator.\u001b[39;00m\n\u001b[0;32m    156\u001b[0m         X_transformed \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean_, (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;241m@\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomponents_\u001b[38;5;241m.\u001b[39mT\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 13.7 MiB for an array with shape (27600, 65) and data type float64"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "n_iters = 1\n",
    "iter_accs = []\n",
    "for i in range(n_iters):\n",
    "    dm.setup()\n",
    "    \n",
    "    fold_accs = []\n",
    "    for fold in range(n_folds):\n",
    "        dm.set_fold(fold)\n",
    "        # print(dm.current_fold)\n",
    "        \n",
    "        # instantiate the model\n",
    "        in_channels = dm.get_data_shape()[-1]\n",
    "        model = CNNTransformer(in_channels, num_classes, d_model, kernel_size, stride, padding,\n",
    "                               n_head, num_layers, dim_fc, dropout, learning_rate)\n",
    "        \n",
    "        # model.current_fold = fold\n",
    "        callbacks = [ModelCheckpoint(monitor='val_loss'), EarlyStopping(monitor='val_loss', patience=es_pat)]\n",
    "        trainer = L.Trainer(max_epochs=max_epochs,\n",
    "                            gradient_clip_val=gclip_val,\n",
    "                            accelerator='auto',\n",
    "                            callbacks=callbacks,\n",
    "                            logger=True,\n",
    "                            enable_model_summary=False,\n",
    "                            enable_progress_bar=False,\n",
    "                           )\n",
    "        trainer.fit(model, dm)\n",
    "        print(trainer.logged_metrics)\n",
    "        trainer.test(model, dm)\n",
    "        fold_accs.append(trainer.logged_metrics['test_acc'])\n",
    "    \n",
    "        # save loss information\n",
    "        # loss_dict = trainer.logger.metrics\n",
    "        # loss_dict['fold'] = fold\n",
    "        # loss_dict['model'] = model\n",
    "    print(f'Averaged accuracy: {sum(fold_accs) / len(fold_accs)}')\n",
    "    iter_accs.append(fold_accs)\n",
    "print(sum(iter_accs) / len(iter_accs), iter_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfc21c01630ca5a",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "fold_accs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
